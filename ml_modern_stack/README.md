## Model ML Stack by TharHtet
In this Chapter I am going teach you following Topics:
- High-Performance Model Training
- ML model Scaling Out
- Model Standardization 
- High-Performance Model Inference



| Feature              | **Triton Inference Server**                  | **Ray Serve**                   | **TensorFlow Serving**    | **Flask**         |
| -------------------- | -------------------------------------------- | ------------------------------- | ------------------------- | ----------------- |
| ğŸ”§ Framework support | ONNX, TensorFlow, PyTorch, XGBoost, TensorRT | Any Python model (custom logic) | TensorFlow only           | Any Python model  |
| ğŸš€ Performance       | âš¡ Extreme (1â€“2 ms latency on GPU)            | Fast with Python + batching     | Moderate                  | âŒ Slow (30â€“100ms) |
| ğŸ§  Auto batching     | âœ… Yes                                        | âœ… Yes                           | âŒ No (needs client logic) | âŒ No              |
| ğŸ” Multi-model       | âœ… Yes                                        | âœ… Yes (via deployments)         | âœ… Limited                 | âŒ No              |
| ğŸ§µ Concurrency       | âœ… CUDA streams + threads                     | âœ… Async tasks                   | âœ…                         | âŒ Single-threaded |
| ğŸŒ Protocols         | HTTP, gRPC                                   | HTTP (FastAPI)                  | gRPC                      | HTTP only         |
| ğŸ“¦ Docker image      | âœ… Official                                   | âœ… (Ray + FastAPI)               | âœ…                         | Manual            |
| ğŸ“Š Monitoring        | âœ… Prometheus + logs                          | âœ… Prometheus                    | Limited                   | âŒ No              |
| ğŸ§© Extensibility     | Some (via backend plugin)                    | âœ… Fully customizable            | âŒ No                      | âœ… Full Python     |
