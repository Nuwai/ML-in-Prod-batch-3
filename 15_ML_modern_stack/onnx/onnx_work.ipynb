{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb029a5b",
   "metadata": {},
   "source": [
    "### ONNX graph reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda5deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer: tf2onnx\n",
      "Opset Version: 13\n",
      "Name: input_image | Shape: [dim_param: \"unk__556\"\n",
      ", dim_value: 224\n",
      ", dim_value: 224\n",
      ", dim_value: 3\n",
      "]\n",
      "Layer 0: Slice (Input: ['input_image', 'const_starts__10', 'const_ends__11', 'const_axes__30'] -> Output: ['functional_1/strided_slice_2:0'])\n",
      "Layer 1: Slice (Input: ['input_image', 'const_ends__11', 'const_starts__22', 'const_axes__30'] -> Output: ['functional_1/strided_slice_1:0'])\n",
      "Layer 2: Slice (Input: ['input_image', 'const_starts__22', 'const_axes__30', 'const_axes__30'] -> Output: ['functional_1/strided_slice:0'])\n",
      "Layer 3: Concat (Input: ['functional_1/strided_slice:0', 'functional_1/strided_slice_1:0', 'functional_1/strided_slice_2:0'] -> Output: ['functional_1/stack_Concat__34:0'])\n",
      "Layer 4: Add (Input: ['functional_1/stack_Concat__34:0', 'functional_1/Squeeze:0'] -> Output: ['functional_1/BiasAdd:0'])\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the model we created earlier\n",
    "model_path = \"skin_cancer_resnet.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# model metadata\n",
    "print(f\"Producer: {model.producer_name}\")\n",
    "print(f\"Opset Version: {model.opset_import[0].version}\") # Crucial for compatibility!\n",
    "\n",
    "# graph inputs\n",
    "for input_node in model.graph.input:\n",
    "    print(f\"Name: {input_node.name} | Shape: {input_node.type.tensor_type.shape.dim}\")\n",
    "\n",
    "# first few layers\n",
    "for i, node in enumerate(model.graph.node[:5]):\n",
    "    print(f\"Layer {i}: {node.op_type} (Input: {node.input} -> Output: {node.output})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853fdf5",
   "metadata": {},
   "source": [
    "### Benchmark mesutring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e950eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a838471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "dummy_data = np.random.randn(32, 224, 224, 3).astype(np.float32)\n",
    "onnx_file = \"skin_cancer_resnet.onnx\"\n",
    "print(f\"Available Providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7606b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(provider_name):\n",
    "    try:\n",
    "        # Load the \"Driver\"\n",
    "        session = ort.InferenceSession(onnx_file, providers=[provider_name])\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        \n",
    "        # Warmup (Get the engine running)\n",
    "        for _ in range(5):\n",
    "            _ = session.run(None, {input_name: dummy_data})\n",
    "            \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        for _ in range(20): # Run 20 times\n",
    "            _ = session.run(None, {input_name: dummy_data})\n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"Provider: {provider_name: <25} | Time: {end - start:.4f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Provider: {provider_name: <25} | Failed (Not installed or HW missing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: CPUExecutionProvider      | Time: 33.5380 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark('CPUExecutionProvider')   # Standard CPU\n",
    "#benchmark('CUDAExecutionProvider')  # NVIDIA GPU But not in Mac (You need onnxruntime-gpu installed for CUDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979dcf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-11-29 11:25:05.130584 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-11-29 11:25:05.131003 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: CoreMLExecutionProvider   | Time: 0.9692 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark('CoreMLExecutionProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4311fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: AzureExecutionProvider    | Time: 33.6219 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark('AzureExecutionProvider')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9db32",
   "metadata": {},
   "source": [
    "### Prepare for Edge devices or model Quantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac5e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "input_model_path = \"skin_cancer_resnet.onnx\"\n",
    "output_model_path = \"skin_cancer_mobile_quant.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=input_model_path,\n",
    "    model_output=output_model_path,\n",
    "    weight_type=QuantType.QUInt8  # Convert weights to 8-bit integers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5ec5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: 89.63 MB\n",
      "Mobile Size:   22.59 MB\n",
      "Reduction:     4.0x smaller!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "original_size = os.path.getsize(input_model_path) / (1024 * 1024)\n",
    "quant_size = os.path.getsize(output_model_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"Original Size: {original_size:.2f} MB\")\n",
    "print(f\"Mobile Size:   {quant_size:.2f} MB\")\n",
    "print(f\"Reduction:     {original_size / quant_size:.1f}x smaller!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f413d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ths_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
