{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1kjn9MiVQNxn"
   },
   "outputs": [],
   "source": [
    "#!pip install crewai crewai_tools langchain_community sentence-transformers langchain-groq langchain_huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jsBQydzLQXsI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set GROQ_API_KEY and TAVILY_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9HjaGfLyRTk-"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from crewai_tools import PDFSearchTool\n",
    "from crewai import Agent, Crew, Task,Process\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "-RfK7vYPRXkk",
    "outputId": "9cebf9c8-f2a2-4f3f-fdd0-de293c4ef275"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "llm =ChatOpenAI(\n",
    "    openai_api_key=os.environ['GROQ_API_KEY'],\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    #model_name=\"llama3-8b-8192\",\n",
    "    model_name=\"llama-3.2-3b-preview\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "# OR Use Ollama to run Locally\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:8b\", #llama3.2:latest\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras\n",
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "e8HGHDddR1Z-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_tool = PDFSearchTool(pdf='attentions_is_all_you_need.pdf',\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"groq\", # or google, openai, anthropic, llama2, ...\n",
    "            config=dict(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                # temperature=0.5,\n",
    "                # top_p=1,\n",
    "                # stream=true,\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"huggingface\", # or openai, ollama, ...\n",
    "            config=dict(\n",
    "                model=\"BAAI/bge-small-en-v1.5\",\n",
    "                #task_type=\"retrieval_document\",\n",
    "                # title=\"Embeddings\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_tool = PDFSearchTool(pdf='attentions_is_all_you_need.pdf',\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"ollama\", # or google, openai, anthropic, llama2, ...\n",
    "            config=dict(\n",
    "                #model=\"llama3-8b-8192\",\n",
    "                model=\"llama3.1:8b\",#\"llama3.2:latest\",\n",
    "                base_url=\"http://localhost:11434\"\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "                provider=\"huggingface\", # or openai, ollama, ...\n",
    "                config=dict(\n",
    "                    model=\"BAAI/bge-small-en-v1.5\",\n",
    "                   \n",
    "                    # title=\"Embeddings\",\n",
    "                ),\n",
    "            ),\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "c9oObmPySjss",
    "outputId": "57568ec3-fabf-4087-e3aa-3f2ae04b88df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Search a PDF's content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Relevant Content:\\nThis makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer\\n\\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. In all but'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How did self-attention mechanism evolve in large language models in pdf?\"\n",
    "rag_tool.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZunpF9DlWGbx"
   },
   "outputs": [],
   "source": [
    "## Using Tavily API Key for search , try with DuckDuck Go\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EaFCl_rBWGY9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://ieeexplore.ieee.org/document/10245906',\n",
       "  'content': 'Transformers, originally introduced for machine translation, and built upon the Self-Attention mechanism, have undergone a remarkable evolution, establishing themselves as the bedrock of large language models (LLMs). Their unparalleled capacity to model intricate relationships and capture extensive dependencies within sequences has propelled their prominence. This article, presented in a popular science format, serves as an introduction to the transformer architecture, elucidating its [...] Opens in a new window Opens an external website Opens an external website in a new window\\n\\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link:    Privacy Policy\\n\\nSelf-Attention and Transformers: Driving the Evolution of Large Language Models | IEEE Conference Publication | IEEE Xplore\\n\\nConferences >2023 IEEE 6th International C... [...] # Self-Attention and Transformers: Driving the Evolution of Large Language Models\\n\\nPDF \"You do not have access to this PDF\")\\n\\nQing Luo; Wei Zeng; Manni Chen; Gang Peng; Xiaofeng Yuan; Qiang Yin\\n\\nAll Authors)\\n\\n Alerts)\\n\\n  # Alerts\\n\\n  Manage Content Alerts\\n\\n  Add to Citation Alerts)\\n\\n Download PDF\\n Download References)\\n Request Permissions\\n Save to\\n Alerts\\n\\n## Abstract:'},\n",
       " {'url': 'https://magnimindacademy.com/blog/the-mechanism-of-attention-in-large-language-models-a-comprehensive-guide/',\n",
       "  'content': 'So this attention mechanism is undoubtedly the backbone of large language models as we know them and has reshaped\\u2002the field of natural language processing (NLP) — changing the way we think about how machines process and generate text. Attention mechanisms have allowed AI to better understand\\u2002and emulate human language, by enabling models to selectively focus on the parts of an input sequence that are most salient. It is the introduction of self-attention, another fundamental component, and [...] At their core, Transformer models\\u2002make use of a self-attention (or intra-attention) mechanism. It allows every word in a\\u2002sentence to reach out, in a sort of interactive way, and decide how much weight one word should give another. This is needed to build up context and relations over tokens in the natural way as it would for the previous\\u2002set of tokens (even across long sequences. \\n\\n#### How Self-Attention Works\\n\\nSeveral stages characterize the\\u2002process of self-attention: [...] This simple realization about attention has become the cornerstone of the models\\u2002that are built on top of that such as state-of-the-art Natural Language Processing (NLP) and self-attention which then get further synthesized into Transformer architectures. So let’s explore\\u2002these concepts in more detail.\\n\\n1. Self-Attention: The Core Process'},\n",
       " {'url': 'https://arxiv.org/abs/2507.19595',\n",
       "  'content': '> Abstract:Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby [...] efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models. [...] enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of'},\n",
       " {'url': 'https://www.ibm.com/think/topics/attention-mechanism',\n",
       "  'content': 'An attention mechanism is a machine learning technique that directs deep learning models to prioritize (or attend to) the most relevant parts of input data. Innovation in attention mechanisms enabled the transformer architecture that yielded the modern large language models (LLMs) that power popular applications like ChatGPT. [...] For an autoregressive, self-attention-based LLM to be capable of translating text, all of the words the model encounters in training—across every language—are learned as part of one large multilingual token vocabulary. The model simply realizes that when a sequence contains instructions like “translate [words in Language 1] into Language 2,” the next words in the sequence should be tokens from Language 2. [...] In self-attention, queries, keys and values are all drawn from the same source. Whereas both Bahdanau and Luong’s attention mechanisms were explicitly designed for machine translation, Cheng at al proposed self-attention—which they called “intra-attention”—as a method to improve machine reading in general. Their attention mechanism, outlined in a 2016 paper, explored not how input elements contribute to an overall sequence, but how different input tokens relate to each other.'},\n",
       " {'url': 'https://medium.com/@kalra.rakshit/introduction-to-transformers-and-attention-mechanisms-c29d252ea2c5',\n",
       "  'content': 'The self-attention mechanism is a pivotal innovation within the Transformer model (probably in human evolution as well?), enabling it to discern intricate relationships within data. It allows the model to weigh the significance of different parts of the input independently of their position in the sequence. This is particularly useful in scenarios where the context is essential for understanding the meaning, such as discerning whether ‘it’ refers to the ‘wolf’ or ‘rabbit’ in a given sentence. [...] Transformers, with their self-attention mechanism, have surpassed traditional models like RNNs and LSTMs in various tasks, particularly in language modeling and text generation. [...] > The self-attention mechanism within transformers allows for the nuanced understanding of language, enabling models to process words in relation to all other words in a sentence, rather than in isolation.\\n\\nThe table below showcases some of the pioneering models in NLP that have leveraged transformer architecture:'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool.run(\"How did self-attention mechanism evolve in large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Snwk1YcWGWe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TKtpRd6TSmco"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def simple_router_tool(question : str) -> str:\n",
    "  \"\"\"Routes queries to the appropriate resources\"\"\"\n",
    "  if \"pdf\" in question.lower():\n",
    "    return \"vectorstore\"\n",
    "  else:\n",
    "    return \"websearch\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xHBRKHGBTkRX",
    "outputId": "77ff5410-ba7a-4564-d3c1-daaa8241bfd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorstore'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = {\"question\": \"What does the  say about machine learning in pdf?\"}\n",
    "simple_router_tool.run(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Hgu7_tdNTzJZ"
   },
   "outputs": [],
   "source": [
    "#Agent 1\n",
    "\n",
    "Router_Agent = Agent(\n",
    "  role='Router',\n",
    "  goal='Route user question to a vectorstore or web search',\n",
    "  backstory=(\n",
    "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
    "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
    "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=llm,\n",
    ")\n",
    "\n",
    "#Task 1\n",
    "router_task = Task(\n",
    "    description=(\"Analyse the keywords in the question {question}\"\n",
    "    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n",
    "    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n",
    "    \"Return a single word 'websearch' if it is eligible for web search.\"\n",
    "    \"Do not provide any other premable or explaination.\"\n",
    "    ),\n",
    "    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n",
    "    \"Do not provide any other premable or explaination.\"),\n",
    "    agent=Router_Agent,\n",
    "    #tools=[simple_router_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLdhxLxtUFGu",
    "outputId": "e4c92b4b-af21-491e-cdb1-f2dc24092b97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 00:11:47,953 - 8410570944 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question What does the  say about machine learning in pdf?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "crew1 = Crew(\n",
    "    agents = [Router_Agent],\n",
    "    tasks = [router_task],\n",
    "    verbose= True,\n",
    ")\n",
    "test_input = {\"question\": \"What does the  say about machine learning in pdf?\"}\n",
    "result1 = crew1.kickoff(inputs=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKzxz1NxU__5",
    "outputId": "d8192c77-cb0e-4cc7-c947-3bd8f27d001c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://ieeexplore.ieee.org/document/10245906',\n",
       "  'content': 'Transformers, originally introduced for machine translation, and built upon the Self-Attention mechanism, have undergone a remarkable evolution, establishing themselves as the bedrock of large language models (LLMs). Their unparalleled capacity to model intricate relationships and capture extensive dependencies within sequences has propelled their prominence. This article, presented in a popular science format, serves as an introduction to the transformer architecture, elucidating its [...] # Self-Attention and Transformers: Driving the Evolution of Large Language Models\\n\\nPDF \"You do not have access to this PDF\")\\n\\nQing Luo; Wei Zeng; Manni Chen; Gang Peng; Xiaofeng Yuan; Qiang Yin\\n\\nAll Authors)\\n\\n Alerts)\\n\\n  # Alerts\\n\\n  Manage Content Alerts\\n\\n  Add to Citation Alerts)\\n\\n Download PDF\\n Download References)\\n Request Permissions\\n Save to\\n Alerts\\n\\n## Abstract: [...] Opens in a new window Opens an external website Opens an external website in a new window\\n\\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link:    Privacy Policy\\n\\nSelf-Attention and Transformers: Driving the Evolution of Large Language Models | IEEE Conference Publication | IEEE Xplore\\n\\nConferences >2023 IEEE 6th International C...'},\n",
       " {'url': 'https://magnimindacademy.com/blog/the-mechanism-of-attention-in-large-language-models-a-comprehensive-guide/',\n",
       "  'content': 'At their core, Transformer models\\u2002make use of a self-attention (or intra-attention) mechanism. It allows every word in a\\u2002sentence to reach out, in a sort of interactive way, and decide how much weight one word should give another. This is needed to build up context and relations over tokens in the natural way as it would for the previous\\u2002set of tokens (even across long sequences. \\n\\n#### How Self-Attention Works\\n\\nSeveral stages characterize the\\u2002process of self-attention: [...] So this attention mechanism is undoubtedly the backbone of large language models as we know them and has reshaped\\u2002the field of natural language processing (NLP) — changing the way we think about how machines process and generate text. Attention mechanisms have allowed AI to better understand\\u2002and emulate human language, by enabling models to selectively focus on the parts of an input sequence that are most salient. It is the introduction of self-attention, another fundamental component, and [...] Example:\\n\\nConsider the sentence:  \\n The cat\\u2002sat on the mat, which was black.”\\n\\nSelf-attention allows\\u2002the model to discern that the word “black” refers to the word “cat”, not the word “mat”. The Query vector of “black” targets\\u2002its relevant pair, Key vector of “cat,” with a high attention weight between them .'},\n",
       " {'url': 'https://ai.plainenglish.io/how-did-attention-mechanisms-revolutionize-large-language-models-8498bb606ef8',\n",
       "  'content': 'Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# How Did Attention Mechanisms Revolutionize Large Language Models?\\n\\nBurak Degirmencioglu\\n\\n9 min read3 days ago\\n\\nThe ability of Large Language Models (LLMs) to understand, generate, and reason with human language stems from a single, powerful concept: the attention mechanism.\\n\\nOriginally a breakthrough for machine translation, attention has become the foundational component of the Transformer architecture, which powers models like GPT and Claude. [...] The attention mechanism is, without question, the engine that powers modern artificial intelligence. Its evolution from a simple weighting mechanism to a complex, highly optimized, and I/O-aware system is what has enabled LLMs to scale. As these models continue to grow, the search for more efficient and powerful attention paradigms will remain the most critical frontier in AI research, defining the next generation of intelligent systems.'},\n",
       " {'url': 'https://www.labellerr.com/blog/evolution-of-neural-networks-to-large-language-models/',\n",
       "  'content': 'The attention mechanism works by assigning different weights to different parts of the input sequence, enabling the model to \"pay attention\" to more relevant words or phrases while making predictions. [...] Neural network models can understand the order of words in language and create sentences that make sense. This helps them do tasks like writing and translating more accurately than older methods.\\n\\nAttention-based approaches, such as the transformer architecture, have lately gained appeal. These models create output by attending to distinct sections of the input sequence using self-attention techniques. [...] Query, Key, and Value: In the context of transformers, the model uses queries, keys, and values to compute attention scores, which help determine which parts of the sequence are most important.\\n Self-Attention: A specific form of attention where the model attends to different words within the same sequence to build contextual relationships.\\n\\n### 7.2 Key Features and Limitations'},\n",
       " {'url': 'https://people.tamu.edu/~sji/classes/Attn-LLM.pdf',\n",
       "  'content': 'HH × W(o) HDv × D = Y N × D X self-attention self-attention ... [...] then normalizes each row independently to sum to one Whereas standard networks multiply activations by fixed weights, here the activations are multiplied by the data-dependent attention coefficients 7 / 41 Self-attention without parameters We can use data matrix X as Q, K, V, along with the output matrix Y, whose rows are given by ym, so that Y = Softmax \\x02 XXT\\x03 X where Softmax [L] takes the exponential of every element of L and then normalizes each row independently to sum to one This process [...] self-attention concat linear Y 12 / 41 Transformer layers Stack multiple self-attention layers on top of each other Introduce residual connections and require the output dimensionality to be the same as the input dimensionality, namely N × D Followed by layer normalization to improves training efficiency Z = LayerNorm [Y(X) + X] Sometimes the normalization layer is applied before the multi-head self-attention as Z = Y ( LayerNorm [X]) + X X multi-head self-attention add & norm MLP add & norm e'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool = TavilySearchResults()\n",
    "question = \"How did self-attention mechanism evolve in large language models in pdf?\"\n",
    "web_search_tool(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "iSP2x6hWV8rv"
   },
   "outputs": [],
   "source": [
    "#Agent 2\n",
    "Retriever_Agent = Agent(\n",
    "role=\"Retriever\",\n",
    "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
    "backstory=(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the information present in the retrieved context to answer the question.\"\n",
    "    \"You have to provide a clear concise answer.\"\n",
    "),\n",
    "verbose=True,\n",
    "allow_delegation=False,\n",
    "llm=llm,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Task 2\n",
    "retriever_task = Task(\n",
    "    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n",
    "    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n",
    "    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n",
    "    ),\n",
    "    expected_output=(\"You should analyse the output of the 'router_task'\"\n",
    "    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n",
    "    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n",
    "    \"Return a claer and consise text as response.\"),\n",
    "    agent=Retriever_Agent,\n",
    "    context=[router_task],\n",
    "    tools=[rag_tool, web_search_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XL-vIr80WsC_",
    "outputId": "66b914be-d2a9-4717-9c66-500e422dbba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 00:18:07,721 - 8410570944 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question How U-Net work?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task extract information for the question How U-Net work? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question \"How U-Net work?\", I need to follow these steps:\n",
      "\n",
      "1. Analyze the output of the 'router_task' to determine which tool to use.\n",
      "2. If the response is 'websearch', use the web_search_tool to retrieve information from the web.\n",
      "3. If the response is 'vectorstore', use the rag_tool to retrieve information from the vectorstore.\n",
      "\n",
      "Since I don't have any information about the output of the 'router_task' yet, let's assume it will be one of these two options.\n",
      "\n",
      "Thought: Let's start by assuming the router task output is 'websearch'.\n",
      "\n",
      "Action: Action\n",
      "Action Input: {\"query\": \"How U-Net work?\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Action' don't exist, these are the only available Actions: Search a PDF's content: A tool that can be used to semantic search a query the attentions_is_all_you_need.pdf PDF's content.\n",
      "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mI will follow the correct steps to provide a clear and concise answer.\n",
      "\n",
      "Since the router task output is assumed to be 'websearch', I need to use the tavily_search_results_json tool. However, I realized that this tool is not suitable for answering questions about technical concepts like U-Net architecture.\n",
      "\n",
      "Let me try again with a different assumption. This time, let's assume the router task output is actually 'vectorstore'.\n",
      "\n",
      "Thought: Since the router task output is 'vectorstore', I will use the Search a PDF's content tool to retrieve information from the vectorstore.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "Action Input: {\"query\": \"U-Net architecture\"}\u001b[0m\u001b[95m \n",
      "\n",
      "Relevant Content:\n",
      "a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. In all but\n",
      "\n",
      "active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016. 10\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "\n",
      "Final Answer:\n",
      "Relevant Content:\n",
      "a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. In all but\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: Relevant Content:\n",
      "a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. In all but\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "# Crew for the System\n",
    "simple_rag_crew = Crew(\n",
    "    agents = [Router_Agent,Retriever_Agent],\n",
    "    tasks = [router_task,retriever_task],\n",
    "    verbose= True,\n",
    ")\n",
    "\n",
    "\n",
    "test_input = {\"question\" :  \"How U-Net work?\"}\n",
    "\n",
    "#test_input = {\"question\" :  \"Does dorthy have any friends?\"}\n",
    "result1 = simple_rag_crew.kickoff(inputs=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TnZ-ZMd4W7Fc",
    "outputId": "0aefbf66-071f-4a3c-b8d9-2b1f9813fbcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qdMEwgfZW96D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question How does self-attention mechanism help large language models in pdf?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task extract information for the question How does self-attention mechanism help large language models in pdf? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find information about how self-attention mechanism helps large language models in PDFs, I need to determine which tool to use.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "Action Input: {\"query\": \"self-attention mechanism and large language models\"}\u001b[0m\u001b[95m \n",
      "\n",
      "Relevant Content:\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6\n",
      "\n",
      "This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer\n",
      "\n",
      "Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: The router task output was 'vectorstore', so I need to use the rag_tool to retrieve information from the vectorstore.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "Action Input: {\"query\": \"self-attention mechanism and large language models\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: Since the router task output was 'vectorstore', I need to use the Search a PDF's content tool to retrieve information from the vectorstore.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "Action Input: {\"query\": \"self-attention mechanism benefits for large language models\"}\u001b[0m\u001b[95m \n",
      "\n",
      "Relevant Content:\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6\n",
      "\n",
      "This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer\n",
      "\n",
      "Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer:\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\n",
      "\n",
      "The Transformer model, based solely on attention mechanisms, has been shown to be superior in quality while being more parallelizable and requiring significantly less time to train.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\n",
      "\n",
      "The Transformer model, based solely on attention mechanisms, has been shown to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\\n\\nThe Transformer model, based solely on attention mechanisms, has been shown to be superior in quality while being more parallelizable and requiring significantly less time to train.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs ={\"question\":\"How does self-attention mechanism help large language models in pdf?\"}\n",
    "\n",
    "#test_input = {\"question\" :  \"Does dorthy have any friends?\"}\n",
    "result2 = simple_rag_crew.kickoff(inputs=inputs)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Agentic_RAGs-b0qwRdDV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
